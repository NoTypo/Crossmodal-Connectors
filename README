LLaVA design
============
Image ----> Clip (768 dim vector) ----> projection matrix W -> V_img 
Prompt ---> Vicuna encoder φ ----> V_text

W and φ are updated.

Data
======
1. Expand COCO(Xv image,Xc caption) to "Human : Xq Xv Assistant : Xc<STOP>" 
where Xq is a set of basic questions like "Describe this image" answer to which is very likely in Xc.
2. Improve the expanded dataset, pass the bounding box coordinates + caption + few shot examples* to gpt4. 

Using above, create 3 types of datasets: 
    - Conversation
    - Detailed descrption
    - Complex reasoning

Input to casual(means autoregressive) LM: [Emb[<s>], Emb[A], Emb[cat], ..., ImgCLIPEmb[features], Emb[.]] 
is produced on the fly at LazySupervisedDataset

# Training
First the projection matrix W is trained so that the clip embeddings of an image are in the same space as the word embeddings.
This is called Stage 1: Pre-training for Feature Alignment and done at pretrain.py

LAION-CC-SBU dataset with BLIP captions
Row 1 in blip_laion_cc_sbu_558k_meta.json 
{
    'url': 'https://ak1.ostkcdn.com/images/products/11932255/P18821274.jpg', 
    'blip_caption': 'a grey watch with an army style strap', 
    'image': '00223/002239345.jpg', 'id': '002239345'
}
Row 1 in blip_laion_cc_sbu_558k.json
{'conversations': [{'from': 'human', 'value': 'Write a terse but informative summary of the picture.\n<image>'}, 
{'from': 'gpt', 'value': 'a grey watch with an army style strap'}], 'image': '00223/002239345.jpg', 'id': '002239345'}

Second is the fine tuning.
head -c 1000 LLaVA-Instruct-150K/llava_v1_5_mix665k.json
[
  {
    "id": "000000033471",
    "image": "coco/train2017/000000033471.jpg",
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nWhat are the colors of the bus in the image?"
      },
      {
        "from": "gpt",
        "value": "The bus in the image is white and red."
      },
    ]
  }

grep -m 50 "bounding box coordinate" LLaVA-Instruct-150K/llava_v1_5_mix665k.json

examples*: They are the only human annotations during data collection used as seed examples when querying GPT-4

Merging image with the text by cur_new_input_embeds.append(cur_image_features).

The image features are flattened so that we can get a single vector to do the dot products across the heads.
image_features = [x.flatten(0, 1) for x in image_features]. Below is the full method
function prepare_inputs_labels_for_multimodal(input_ids, 
                                            position_ids, 
                                            attention_mask, 
                                            past_key_values, 
                                            labels, images, 
                                            image_sizes):

Sources
=======
Default size of clip embeddings (768) is used: https://github.com/huggingface/transformers/blob/85345bb439652d3f03bb4e123cef7a440f2ba95b/src/transformers/models/clip/configuration_clip.py#L163

