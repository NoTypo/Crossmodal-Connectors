# Crossmodal-Connectors

## Data prep

1. Expand COCO(Xv image,Xc caption) to "Human : Xq Xv<STOP> Assistant : Xc<STOP>" where Xq is a set of basic questions like "Describe this image" answer to which is very likely in Xc.
2. Improve the expanded dataset, pass the bounding box coordinates + caption + few shot examples* to gpt4. This is done to create 3 types of datasets: 
    - Conversation
    - Detailed descrption
    - Complex reasoning


examples*: They are the only human annotations during data collection used as seed examples in in-context-learning to query GPT-4